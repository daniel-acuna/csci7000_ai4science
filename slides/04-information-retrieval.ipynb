{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fantastic-nepal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSCI 7000 - AI for Science\n",
    "## Week 4\n",
    "### Information retrieval: search systems\n",
    "- Daniel E. Acuna, Department of Computer Science, University of Colorado, Boulder\n",
    "\n",
    "Credit to professor Jaime Arguello (https://ils.unc.edu/courses/2018_spring/inls509_001/)\n",
    "\n",
    "Reference: _Croft, W. B., Metzler, D., & Strohman, T. (2010). Search engines: Information retrieval in practice (Vol. 520, pp. 131-141). Reading: Addison-Wesley._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-russia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/information_retrieval/covid19_google_scholar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-temple",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/information_retrieval/covid19_google.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-homework",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is information retrieval?\n",
    "\n",
    "> Information retrieval (IR) is a field concerned with the design, development, and evaluation of interactive systems that help users find information.  \n",
    "\n",
    "_Schütze, H., Manning, C. D., & Raghavan, P. (2008). Introduction to information retrieval (Vol. 39, pp. 234-65). Cambridge: Cambridge University Press._\n",
    "\n",
    "- **Information retrieval systems are not mostly called _search engines_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-studio",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is information retrieval\n",
    "\n",
    "- Given a **query** and a **corpus**, find **relevant** items:\n",
    " - **query**: a user's expression of their information need\n",
    " - **corpus**: a repository of retrievable items\n",
    " - **relevance**: satisfaction of the user's information need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-coupon",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"query\", \"corpus\", \"relevance\"?\n",
    "![](images/information_retrieval/covid19_google_scholar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-circle",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why is IR difficult?\n",
    "- Users don't know what they want\n",
    "- Users don't know how to express what they want as a \"query\"\n",
    "- Computers can't understand NLP well\n",
    "- Relevance contains multiple objectives (e.g., similarity to query, recent, local, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-matthew",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why is IR difficult?\n",
    "- From competition TREC 2005 HARD Track\n",
    "- Query 435: \"curbing population growth\"\n",
    "- Description by user: \n",
    "> What measures have been taken worldwide\n",
    "> and what countries have been effective in curbing\n",
    "> population growth? A relevant document must describe\n",
    "> an actual case in which population measures have been\n",
    "> taken and their results are known. Reduction measures\n",
    "> must have been actively pursued. Passive events such as\n",
    "> decease, which involuntarily reduce population, are not\n",
    "> relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-greeting",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We are not going to focus on the following\n",
    "- Item acquisition: crawling, feeds, deep web (Bing.com takes two weeks to scan the web)\n",
    "- Storage\n",
    "> The Google Search index contains hundreds of billions of webpages and is well over 100,000,000 gigabytes in size. It’s like the index in the back of a book — with an entry for every word seen on every webpage we index. When we index a webpage, we add it to the entries for all of the words it contains.\n",
    "> https://www.google.com/search/howsearchworks/how-search-works/organizing-information\n",
    "\n",
    "- Index creation\n",
    "- Index compression\n",
    "- Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-reduction",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Architecture of a search engine\n",
    "- Index construction\n",
    "![](images/information_retrieval/indexing_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-inspection",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Steps of search engine\n",
    "\n",
    "- Text acquisition:\n",
    "    - Document (PDF, HTML, DOC) needs to be converted into some standard structure to be stored.\n",
    "- Text transformation:\n",
    "    - The acquired document needs to be transformed before it is indexed. Usually, this requires:\n",
    "        - Parser\n",
    "        - Stopping\n",
    "        - Stemming\n",
    "        - Link analysis\n",
    "        - Information extraction\n",
    "        - Classifier\n",
    "- Index creation:\n",
    "    - Tranformed text goes through some or all of these steps\n",
    "        - Document statistics\n",
    "        - Weighting\n",
    "        - Inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-hanging",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text transformation\n",
    "- Parser: responsible for processing the sequence of text tokens in the document to recognize structural elements such as titles, figures, links, and headings.\n",
    "    - Is \"apple\" the same as \"Apple\"? is \"on-line\" two words or one word? Should we treat the apostrophe \"O'Connor\" the same as in \"owner's\"?\n",
    "- Stopping: remove stop words\n",
    "- Stemming: word-level transformation. For example, group \"fish\", \"fishes\", \"fishing\" into one term. \n",
    "- Link extraction: for html documents, extract links and title of links. Links should be treated differently\n",
    "- Information extraction: extract syntacting features or part-of-speech tags.\n",
    "- Classifer: topical categories of documents such as \"sports\", \"politics\", and \"business\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-attribute",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Index creation\n",
    "- Document statistics: gather and record statistical information information about words, features, and documents. Count of index term occurrences in individual documents, and position in document where index terms ocurrs, and counts of a term across documents\n",
    "- Weighting: Relative importance of words in documents, used to compute scores for ranking. For example, tf-idf (term frequency inverte document frequency)\n",
    "- Inversion: from document-term to term-document indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-ethics",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Architecture of a search engine\n",
    "\n",
    "- Query process\n",
    "![](images/information_retrieval/query_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-compression",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# User interaction\n",
    "\n",
    "- Query input: what the user enters. Typically this is free text, but it can be more complicated. For example, some search engines allow for time span, location, etc\n",
    "- Query transformation: spell checking and query suggestion\n",
    "- Result output: snippets summarizing results with highligthing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-linux",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ranking\n",
    "- Scoring: calculates the score of a document using a _ranking algorithm_, which is based on a _retrieval model_. The basic ranking is\n",
    "\n",
    "$$\\sum_i q_i d_i$$\n",
    "where $q_i$ is the query term weight of the $i$th term, and $d_i$ is the document term weight. Term weights are particular to the retrieval model, but are similar to tf-idf weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-digit",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "- Logging\n",
    "- Ranking analysis\n",
    "- Performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-algebra",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Retrieval models\n",
    "\n",
    "- Overview\n",
    "- Probabilisitc models\n",
    "- Ranking based on a language models\n",
    "- Machine learning and information retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-gospel",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Retrieval models: Boolean retrieval\n",
    "- Also known as exact match.\n",
    "    - `lincoln`\n",
    "    - `lincoln AND president` => \"Ford Motor Company today announced that Darryl Hazel will succeed Brian Kelley as __president__ of __Lincoln__ Mercury.\"\n",
    "    - `lincoln AND president AND NOT (automobile OR car)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-swiss",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The vector space model\n",
    "\n",
    "![](images/information_retrieval/term_document_matrix.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-voltage",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The vector space model\n",
    "- Queries are represented in the same space as documents\n",
    "$$Q = (q_1, q_2, \\dots, q_t)$$\n",
    "\n",
    "where $q_i$ is the weight of the $i$th term.\n",
    "![](images/information_retrieval/vector_representation_doc_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-passenger",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The vector space model\n",
    "- Ranking can be done by computing the distance between query and document. For example, the cosine distance\n",
    "\n",
    "![](images/information_retrieval/cosine_distance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-mistress",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The vector space model\n",
    "- Example: Weight of vector could be tf-idf\n",
    "- Weight of query could be based on Rocchio algorithm:\n",
    "![](images/information_retrieval/rocchio_algorithm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-dominican",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Information retrieval as classification\n",
    "\n",
    "![](images/information_retrieval/ir_as_classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-stanford",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IR as classification\n",
    "\n",
    "- We can usually estimate $p(D | R)$ based on a set of relevant documents. For example,\n",
    "\n",
    "$$p(\\text{lincoln}, \\text{president} | R) = p(\\text{lincoln} | R) p(\\text{president} | R)$$ assuming independence\n",
    "- We can use Bayes' theorem to calculate \n",
    "$$p(R | D) = \\frac{p(D | R) p(R)}{p(D)}$$\n",
    "- Therefore we return a document as relevant if\n",
    "$$p(R | D) > p(NR | D)$$\n",
    "which is\n",
    "$$\\frac{p(D|R)}{p(D|NR)} > \\frac{p(NR)}{p(R)},$$ where the left-hand side is the likelihood ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-straight",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IR as classification\n",
    "\n",
    "- We assume that term $i$ has a $p_i$ probability of appearing in the relevant set and $s_i$ probability of appearing in the non-relevant set\n",
    "\n",
    "$$\\frac{p(D|R)}{p(D|NR)} = \\prod_{i:d_i=1} \\frac{p_i}{s_i} \\prod_{i:d_i=0} \\frac{1-p_i}{1-s_i}$$\n",
    "after some manipulation we can compute the log-likelihood ratio as\n",
    "$$\\sum_{i:d_i = 1} \\log \\frac{p_i (1-s_i)}{s_i (1-p_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-theorem",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IR as classification\n",
    "- But what about the query? We could compute the ratio only for the terms matching between the document and the query\n",
    "- A further assumption can make us justify tf-idf. We could assume that $p_i$ is constant, say 0.5, and that $s_i$ is simply the probability of the term appearing in the whole collection. That would simplify the log-likelihood ratio to\n",
    "\n",
    "$$\\log \\frac{p_i (1-s_i)}{s_i (1-p_i)} = \\log \\frac{0.5 (1-n_i/N)}{n_i/N (1 - 0.5)} \n",
    "= \\log \\frac{(1-n_i/N)}{n_i/N}\n",
    "= \\log \\frac{N-n_i}{n_i}$$\n",
    "which is similar to tf-idf but makes tf a binary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-society",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IR as classification\n",
    "- We could estimate $p_i = r_i / R$, which is the number of relevant documents that contain a term divided by the total number of relevant documents\n",
    "- We could estimate $s_i = (n_i - r_i)/(N-R)$ which is the complement\n",
    "- It could happen that $r_i$ is zero so we avoid by _smoothing_ it with $p_i = (r_i + 0.5)/(R+1)$ which makes $s_i = (n_i - r_i+0.5)/(N-R+1)$\n",
    "- The new scoring is\n",
    "$$\\sum_{i:d_i = 1} \\log \\frac{(r_i + 0.5)/(R-r_i+0.5)}{(n_i-r_i+0.5)/(N-n_i-r+r_i+0.5)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-animal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BM25 ranking algorithm\n",
    "- Ignoring the tf is bad for ranking.\n",
    "- BM25 extends the classification algorithm to include document and query term weights. It is based on experiment and not formal\n",
    "$$\\sum_{i \\in Q} \\log \\frac{(r_i + 0.5)/(R-r_i+0.5)}{(n_i-r_i+0.5)/(N-n_i-r+r_i+0.5)}\n",
    "\\frac{(k+1)f_i}{K+f_i}\\frac{(k_2+1)qf_i}{k_2+qf_i}$$\n",
    "where $f_i$ is the frequency of the term in the document $qf_i$ is the frequency of the term in the query, and $k_1$, $k_2$, and $K$ are set empirically\n",
    "- Typically $k_1 = 1.2$, $k_2 \\in [0, 1,000]$ and \n",
    "$$K = k_1 ((1-b) + b \\frac{dl}{avdl}),$$ where $b$ is a parameter, $dl$ is the document length, and $avdl$ is the average length of a document in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-conspiracy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ranking based on language models\n",
    "- We assume a probability generation of documents based on a model of how language is generated\n",
    "\n",
    "$$ D \\sim p(D)$$\n",
    "- We can use this distribution to compute the likelihood of a document given a query\n",
    "$$p(D | Q) \\propto p(Q|D) p(D),$$ where usually $p(D)$ is uniform.\n",
    "- Further assuming independence among terms in the query, we have\n",
    "$$p(Q|D) = \\prod_i p(q_i | D) \\approx \\prod_i \\frac{f_{q_i,D}}{|D|}$$.\n",
    "- We can smooth this quantity and tkae th log, leading to\n",
    "$$\\log p(Q|D) = \\sum_i \\log (1-\\lambda) \\frac{f_{q_i,D}}{|D|} + \\lambda \\frac{c_{q_i}}{|C|}$$, where $c_{q_i}$ is the frequency of term in a collection langauge (large set of $C$ documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-cable",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning and information retrieval\n",
    "- We want to learn a function that ranks documents for a given query\n",
    "- In principle, we could _learn_ the tfidf or BM25 weighting schemes from data.\n",
    "- Imagine we have tons of data for a query $q$ and a document $d$ and whether or not it is relevant $y=1$ or $y=0$\n",
    "- Simplest approach: pointwise approach\n",
    "  - Extract features from pair of query and document $x(q, d)$ and learn a function\n",
    "  $$p(y=1 | x(q, d))$$\n",
    "  - The features could be anything (e.g., set of distances between query and document, overlap between query and title of document, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-attack",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning and information retrieval\n",
    "- Standard set of features\n",
    "\n",
    "![](images/information_retrieval/ltor_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-italy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PageRank\n",
    "- It could be used as another feature\n",
    "- Based on \"citation\" among web pages. It favors web pages that are linked to by many other web pages\n",
    "- Hard to *hack*\n",
    "- Based on stationary distribution of a random walk across the web.\n",
    "![](images/information_retrieval/pagerank.jpg)\n",
    "\n",
    "https://www.youtube.com/watch?v=CsvyPNdQAHg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-constitutional",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with point-wise approach\n",
    "- It does not take into account the ordering of preferences\n",
    "- Often, users do not have a clear definition of relevant and irrelevant, but can give feedback when comparing two results\n",
    "- This is used by a pairwise approach. We have pairs of documents $d_i$ and $d_k$ for each given query $q$ where we know assume that the relevance of one is greater than the other $r(d_j, q) > r(d_k, q)$ and $y_{jk} = 1$, or not greater than another where $y_{jk} = 0$.\n",
    "- Therefore, we learn\n",
    "$$p(y_{jk} | x(q, d_j), x(q, d_k)),$$ where $x$ computes a feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-quilt",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pairwise approach\n",
    "- We could often assume that there is a _latent_ variable that maps the feature sets into a relevance score, and that the difference between this variable indicates who much more relevant one document is compared to the other\n",
    "$$p(y_{jk} | x_j, x_k) = \\sigma(f(x_j) - f(x_k)), $$ where $f$ is some function such as a simple linear function $f(x) = w^T x$ known as RankNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-assembly",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Listwise approach\n",
    "- We could take the pairwise approach one step further and we could provide a ranking of documents for a given query based on relevance\n",
    "- The training data would be, for a given query, a set of documents $d_1, \\dots, d_m$ where the score of document 1, $s_1$ would be higher than $s_2$, score of $s_2$ would be higher than $s_3$, and so on.\n",
    "- Uncertainty about a permutation can be modeled with the Plackett-Luce distribution\n",
    "$$p(\\pi | s) = \\prod_{j=1}^m \\frac{s_j}{\\sum_{u=j}{m} s_u}$$\n",
    "- For example, for a permutation $\\pi = (A, B, C)$ and scores $s_A, s_B, s_C$ the distribution would look like. \n",
    "$$p(\\pi | s) = \\frac{s_A}{s_A + s_B + s_C} \\frac{s_B}{s_B + s_C} \\frac{s_C}{s_C}$$\n",
    "- The log-likehood for a given set of scores computes scores over all possible permutations, which in intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-split",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Listwise approach\n",
    "\n",
    "- We can simplify further if we assume that only one document is relevant (document $y_i = c$ for training data ranking $i$)\n",
    "- We can generalize then the softmax of pointwise approach to be a multinomial probability\n",
    "$$p(y_i = c | x) = \\frac{\\exp(s_C)}{\\sum_{c' = 1}^m \\exp (s_{c'})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-abraham",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "- **Mean reciprocal rank (MRR)** For a query $q$, let the rank of the first relevant document be $r(q)$. MRR is the average of $1/r(q)$ across all queries\n",
    "- **Mean average precision (MAP)**. When we have binary relevance labels. Precision at $k$ is defined as\n",
    "$$P@k = \\frac{\\text{number of relevant documents in the top $k$ positions of the ranking}}{k}$$\n",
    "then, average precision for a ranking is\n",
    "$$AP = \\frac{\\sum_{k \\in \\text{relevant}} P@k}{\\text{number of relevant docs}}$$\n",
    "Then **MAP** is the average across all queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-remark",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation (2)\n",
    "- **Normalized discounted cumulative gain (NDCG)** Here, we have feedback $r_i$ on the relevance of each item $i$. **Discounted cumulative gain** for the first $k$ items is\n",
    "$$DCG@k(r) = r_1 + \\sum_{i=2}^k \\frac{r_i}{\\log_2 i}$$\n",
    "The problem with DCG is that it inflates the score with longer results (more documents returns). Therefore, it is normalized by the \"idealized\" ranking.\n",
    "$$IDCG@k(r) = \\arg \\max_\\pi DCG@k(r)$$ which is simply sorting the documents by the feedback $r_1, \\dots, r_m$. The **NDCG** is then $DCG/IDCG$\n",
    "\n",
    "![](images/information_retrieval/ndcg_example.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
