{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1>Unit 8</h1> </center>\n",
    "<center> <h1>Case Study 2: Recommendation Systems</h1></center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center> <h3>IST 718 – Big Data Analytics</h3> </center>\n",
    "<center> <h3>Daniel E. Acuna</h3> </center>\n",
    "<center> <h3>http://acuna.io</h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised learning\n",
    "- Supervised learning is well understood:\n",
    "  - We observe features $X$ and the associated outcome $Y$.\n",
    "  - Depending on $X$ and $Y$, we know which method to apply (e.g., linear regression or Logistic regression.)  \n",
    "  \n",
    "- In unsupervised learning, we only observe $X$—we do not have any associated outcome $Y$.  \n",
    "\n",
    "- Unsupervised learning is more subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goals of unsupervised learning\n",
    "- Most data is unsupervised and therefore easier to acquire.  \n",
    "\n",
    "- Unsupervised learning  helps to *understand* the data as part of *exploratory data analysis*.  \n",
    "\n",
    "- Examples:\n",
    "  - Subgroups of breast cancer patients grouped by their gene expression measurements.\n",
    "  - Groups of shoppers characterized by their browsing and purchase histories.\n",
    "  - Movies grouped by the ratings assigned by movie viewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use case: a recommendation system\n",
    "- This problem seeks to find similarities between what a user or set of users have liked in the past with new items never seen before.  \n",
    "\n",
    "- This problem can be casted as an unsupervised learning approach if we seek to map high dimensional data into low dimensional data.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca1.png\" width=\"35%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use case: a recommendation system (2)\n",
    "- This problem seeks to find similarities between what a user or set of users have liked in the past with new items never seen before.  \n",
    "\n",
    "- This problem can be casted as an unsupervised learning approach if we seek to map high dimensional data into low dimensional data.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca2.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use case: a recommendation system (3)\n",
    "- This problem seeks to find similarities between what a user or set of users have liked in the past with new items never seen before.  \n",
    "\n",
    "- This problem can be casted as an unsupervised learning approach if we seek to map high dimensional data into low dimensional data.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca3.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use case: a recommendation system (4)\n",
    "- This problem seeks to find similarities between what a user or set of users have liked in the past with new items never seen before.  \n",
    "\n",
    "- This problem can be casted as an unsupervised learning approach if we seek to map high dimensional data into low dimensional data.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca4.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use case: a recommendation system (5)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca5.png\" width=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality reduction: why?\n",
    "- Performing nearest neighbor search in high dimensions is really hard.  \n",
    "\n",
    "- **Basically, the distance between close and far points becomes smaller and smaller with the number of dimensions!**  \n",
    "  - See: Beyer K., Goldstein J., Ramakrishnan R., Shaft U. (1999). When Is \"Nearest Neighbor\" Meaningful?  \n",
    "  \n",
    "- Let’s try:\n",
    "  - A unit ball from -1 to 1 in $n$ dimensions.  \n",
    "  \n",
    "- Average distance between two random points as a function of $n$: what is your intuition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality reduction: why? (2)\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca6.png\" width=\"50%\" align=\"center\"></center>  \n",
    "$$V(n)=\\frac{\\pi^{\\frac{n}{2}}}{\\Gamma\\left(\\frac{n}{2} + 1 \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality reduction\n",
    "- Dimensionality reduction methods seek to reduce the dimensions of an input dataset while trying to maintain properties of the original dataset.  \n",
    "\n",
    "- Dimensionality reduction can be used when we do not wish to perform variable selection in regression:\n",
    "\n",
    "$$y = \\theta_0 + \\sum x_i\\theta_i$$  \n",
    "\n",
    "$\\qquad$to  \n",
    "\n",
    "$$y = \\theta_0 + \\sum z_i\\beta_i$$  \n",
    "<br>\n",
    "<center>where $z$ is a transformation of the original data $x$.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality reduction (2)\n",
    "- We do dimensionality reduction because the distance between elements is meaningful—*really different* elements should be *really far apart*.  \n",
    "\n",
    "- Typical distances between vectors:\n",
    "  - Euclidian:  \n",
    "  $$d(v,w) = \\sum(v_i-w_i)^2$$  \n",
    "  \n",
    "  - Manhattan distance:  \n",
    "  $$d(v,w) = \\sum \\mid v_i-w_i \\mid$$  \n",
    "  \n",
    "  - Cosine distance:  \n",
    "  $$d(v,w)=\\frac{v \\cdot w}{\\|v\\|\\|w\\|}=\\frac{\\sum v_iw_i}{\\sqrt(\\sum v_i^2) \\sqrt(\\sum w_i^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis\n",
    "- Produces a low dimensional representation of the original data.  \n",
    "\n",
    "- It does so by producing a sequence of linear combinations of the data with maximum variance.  \n",
    "\n",
    "- Each linear combination is uncorrelated with the other ones.  \n",
    "\n",
    "- PCA also serves as a tool for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA: Math\n",
    "- The **first principal component** of a set of features $X$ is the normalized linear combination of the features:  \n",
    "\n",
    "$$Z_1 = \\phi_{11}x'_1 + \\phi_{21}x'_2 + \\cdots + \\phi_{m1}x'_m$$  \n",
    "\n",
    "$\\qquad$where normalized means that  \n",
    "  \n",
    "  $$\\sum\\phi_{i1}^2 = 1 \\quad\\text{and}\\quad x'_i = x_i - \\overline{x}_i$$  \n",
    "  \n",
    "$\\qquad$such that **the variance of Z1 is maximal**.  \n",
    "  \n",
    "\n",
    "- We refer to the vector $\\phi_{i1}$ as the **loading** of the first principal component. \n",
    "\n",
    "- In principle the normalization constraint is only for constraining the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA: Math (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca7.png\" width=\"45%\" align=\"center\"></center>  \n",
    "\n",
    "$\\qquad Z_1 = \\phi_{11}x'_1 + \\phi_{21}x'_2 + \\cdots + \\phi_{m1}x'_m \\qquad\\qquad\\ Z_1$ with maximum variance.  \n",
    "<br>\n",
    "<center>Can you find ${\\large\\mathbf{\\phi_{i1}}}$?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA: Math (3)\n",
    "- The second principal component needs to be uncorrelated with the first one (orthogonal.)  \n",
    "\n",
    "- This can be done by first projecting the data into the first component, and then finding the second component on the projected data.  \n",
    "\n",
    "- This ensures that the second component is uncorrelated with the first one.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca8.png\" width=\"40%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA: All components\n",
    "- If we do this multiple times, we will obtain $m$ principal components, each with smaller and smaller variance.  \n",
    "\n",
    "- We can use this variance to understand how much the principal components **explain the variance** in the data.  \n",
    "\n",
    "- This is, if we compute the variance of each of the components, we will see that they decrease:  \n",
    "$$\\text{var}(Z_1) > \\text{var}(Z_2) > \\cdots >\\text{var}(Z_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA: Running it on diabetes dataset\n",
    "- PCA is available in `pyspark.ml.feature`  \n",
    "\n",
    "- Running it on diabetes dataset on the variables age, sex, BMI, and glucose and plotting first two PCs.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca9.png\" width=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA: Running it on diabetes dataset (2)\n",
    "- These are the first two principal components (columns are normalized.)  \n",
    "\n",
    "- How to interpret?  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca10.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA: Running it on diabetes dataset (3)\n",
    "- These are the first two principal components (columns are normalized.)  \n",
    "\n",
    "- How to interpret?  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca11.png\" width=\"100%\" align=\"center\"></center>\n",
    "**Biggest absolute value is most important!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear algebra\n",
    "- Let’s assume we have standardized the input data as follows:  \n",
    "\n",
    "$$x'_i = x_i - \\overline{x}_i$$  \n",
    "\n",
    "- Then we can represent the first $p$ principal components as a matrix  \n",
    "\n",
    "$$\\Theta_{m \\times p}$$  \n",
    "$\\qquad$and project the data into the principal components:  \n",
    "\n",
    "$$Z=X'\\Theta$$  \n",
    "\n",
    "- The matrix of principal components can be seen as a linear function that maps objects from one space ($X'$) into a another space ($Z$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear algebra (2)\n",
    "- This connection is important when we try to understand what the principal components are capturing.  \n",
    "\n",
    "- The inverse of the linear function is represented by the transpose of $\\Theta$.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca12.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: MNIST digit dataset\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca13.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: MNIST digit dataset\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca14.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: MNIST digit dataset (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca15.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: MNIST digit dataset (3)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca16.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: MNIST digit dataset (4)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca17.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Using 1 PC\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca18.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Using 2 PCs\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca19.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Using 3 PCs\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca20.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Using 4 PCs\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca21.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Using 10 PCs\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca22.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Using 20 PCs\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca23.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Using 30 PCs\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca24.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Using 40 PCs\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca25.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Variance of Z columns\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca26.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Variance of Z columns (as % of total)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca27.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to choose number of components?\n",
    "- No good answer!  \n",
    "\n",
    "- Typical answer is to look for an \"elbow\" in variance explained plot.  \n",
    "\n",
    "- Others choose the least number of components to explain 95% of the variance.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca28.png\" width=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use case: suggesting courses\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca29.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using PCA to model courses\n",
    "- Apply dimensionality reduction and suggest based on nearest neighbors in lower dimension.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca30.png\" width=\"70%\" align=\"center\"></center>\n",
    "<br>\n",
    "- But how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA on TF-IDF matrix for course description\n",
    "- Use PCA on TF-IDF matrix to map courses to a low dimensional space.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca31.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Biggest absolute loadings of PCA\n",
    "- Absolute value of loadings:  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca32.png\" width=\"100%\" align=\"center\"></center>\n",
    "<br>\n",
    "- Any interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now, lets recommend courses with 10 PCs\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca33.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple “likes”?\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca34.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple “likes”? (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca35.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix factorization\n",
    "- In general, the approximation approach seen so far can be casted as a matrix factorization problem.\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca36.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix factorization (2)\n",
    "- The goal is to find $U$ and $V$ so as to minimize the error in the reproduction of $X$.  \n",
    "\n",
    "- In PCA, $U$ represents the loadings and $V$ represents the transpose of $U$.  \n",
    "\n",
    "- In other approaches, $U$ and $V$ have more complex relationships.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca37.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "- LDA is a very popular language modeling technique.  \n",
    "\n",
    "- Each document ($x$) is represented as a bag-of-words. (*)  \n",
    "\n",
    "- $U$ represents the distribution of topics of each document.  \n",
    "\n",
    "- $V$ represents the distribution of words per topic.\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca37.png\" width=\"70%\" align=\"center\"></center>\n",
    "<br>\n",
    "<sup>* The probabilistic generation of a document is more complicated than this.</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA) (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca38.png\" width=\"70%\" align=\"center\"></center>\n",
    "<br>\n",
    "<center><sup>From: Probabilistic Topic Models By David M. Blei, Communications of the ACM, Vol. 55 No. 4\n",
    "</sup></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA) (3)\n",
    "- LDA makes stronger assumptions about the distributions as they need to be probabilities.  \n",
    "\n",
    "- It always generates positive counts.  \n",
    "\n",
    "- This is related to non-negative matrix factorization, where $U$ and $V$ are forced to be positive.\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca37.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA) (4)\n",
    "- How LDA works internally is beyond the scope of this course, but we can still examine the fits.  \n",
    "\n",
    "- In Spark, this is inside `clustering`.  \n",
    "\n",
    "- We need to specify the number of topics. Let’s say 5:  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca40.png\" width=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA) (5)\n",
    "- The transformation of a document will give its distribution of topic.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca41.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA) (6)\n",
    "- The transformation of a document will give its distribution of topic.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca42.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA: choosing number of topics\n",
    "- Again, validating LDA is difficult.  \n",
    "\n",
    "- Choose number of topics is an active area of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering: K-means\n",
    "- We could try to put similar courses together.  \n",
    "\n",
    "- We suggest based on the cluster where a course belongs.  \n",
    "\n",
    "- *K-means* is an algorithm that finds $k$ centroids in the data.  \n",
    "\n",
    "- It is available in `clustering` as `KMeans`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca43.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering: K-means (2)\n",
    "- It is available in `clustering` as `KMeans`.  \n",
    "\n",
    "- Algorithm is highly dependent on initial conditions.  \n",
    "\n",
    "- Unreliable!\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca44.png\" width=\"100%\" align=\"center\"></center>  \n",
    "**<center>All other courses are in one big cluster!</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Idea: define a low level representation of a course’s features\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca45.png\" width=\"100%\" align=\"center\"></center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Idea: define a similarity function between spaces and a general similarity function\n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca46.png\" width=\"80%\" align=\"center\"></center>  \n",
    "$$D(a,b) = \\alpha D_\\text{description}(a,b) + \\beta D_\\text{level1}(a,b) + \\gamma D_\\text{level2}(a,b) + \\eta D_\\text{credit}(a,b)$$  \n",
    "\n",
    "We should enforce that:  \n",
    "$$0<\\alpha,\\beta,\\gamma,\\eta<1 \\quad\\text{and}\\quad \\alpha+\\beta+\\gamma+\\eta=1$$  \n",
    "\n",
    "\n",
    "<center>Example similarity functions?</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Idea: define a similarity function between spaces and a general similarity function (2)\n",
    "- How to choose parameters?  \n",
    "\n",
    "<br>\n",
    "$$D(a,b) = \\alpha D_\\text{description}(a,b) + \\beta D_\\text{level1}(a,b) + \\gamma D_\\text{level2}(a,b) + \\eta D_\\text{credit}(a,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can we do better?\n",
    "- In principle, we can build a recommendation system based on supervised data.  \n",
    "\n",
    "- Discuss how we might be able to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using logistic regression to suggest courses\n",
    "- In principle, we can build a recommendation system based on supervised data.  \n",
    "\n",
    "- If we have a set of courses that a user found relevant and irrelevant, then we can predict the vote based on the model.  \n",
    "\n",
    "<br>\n",
    "$$z_{a,b} = \\alpha D_\\text{description}(a,b) + \\beta D_\\text{level1}(a,b) + \\gamma D_\\text{level2}(a,b) + \\eta D_\\text{credit}(a,b)$$  \n",
    "\n",
    "<br>\n",
    "$$p(y \\mid a,b) = \\frac{1}{1+\\exp(-z_{a,b})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other alternatives?\n",
    "- So far,  recommendations are based on what the user likes, but can we use what other people have liked?  \n",
    "\n",
    "- Our data would be, for a list of students, the courses they have taken.  \n",
    "\n",
    "- Then, we could apply dimensionality reduction techniques to reduce that.  \n",
    "\n",
    "- This approach is known as **collaborative filtering** while the previous approaches are known as **content-based filtering**.  \n",
    "\n",
    "<br>\n",
    "<center><sup>Achakulvisut T, **Acuna DE**, Ruangrong T, Kording K (2016) Science Concierge: A Fast Content-Based Recommendation System for Scientific Publications. PLoS ONE 11(7)</sup></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Collaborative filtering\n",
    "- What would the matrix $X$ be? And $U$ and $V$?  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca37.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Collaborative filtering (3)\n",
    "- What would the matrix $X$ be? And $U$ and $V$?  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-08/unit-08-0_csrspca47.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Collaborative filtering (3)\n",
    "- For new users, we ask them about a set of interesting courses and not interesting courses.  \n",
    "\n",
    "- Given that we have learned $V$ from other users, we try to find the best user preference vector $u$ for that particular user.  \n",
    "\n",
    "- In particular, if we use a least square prediction, then we want to find the $u$ such that:  \n",
    "\n",
    "$$\\arg\\max_u \\sum \\left(x_{ij}-u^Tv_j\\right)^2 + \\lambda\\sum u_p^2$$  \n",
    "\n",
    "- This is implemented in Spark in `recommendation` as `ALS`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
